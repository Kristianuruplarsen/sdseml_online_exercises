{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Networks 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "Networks are mathematical representations of complex systems. We can use networks to gain various statistical insight about the system we're representing, and we can look for patterns at the meso-scale by employing *community detection* algorithms. This week we will explore the following:\n",
    "\n",
    "* Network null models\n",
    "* How to use a null model to infer the p-value of a result\n",
    "* How a popular community detection algorithm works (and fails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Outside of class, use [issue on GitHub](https://github.com/abjer/tsds/issues) for asking questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T10:56:27.700322Z",
     "start_time": "2019-03-12T10:56:27.695950Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import networkx as nx           # `pip install networkx`\n",
    "import json\n",
    "from collections import Counter\n",
    "import community                # `pip install python-louvain` or `conda install -c auto python-louvain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Network null models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a null model?** Null models are alternative instances of data, that are used to assess the amount of signal that is due to pure randomness. For example, you might have\n",
    "measured some signal in your data, like a large number of triangles (high average local clustering coefficient)\n",
    "in your network, but before you go and report that to your boss you need to answer one crucial question: how does\n",
    "this result compare with a *random* one? In other words, how can you be so sure that this high number of triangles\n",
    "measured is not perfectly normal even in a random network of similar origin and therefore not very special after all? The answer: **you\n",
    "create a null model to compare your result with!**\n",
    "\n",
    "> Note: null models are a concept from the general field of statistics and therefore not just specific to\n",
    "networks. You can use this statistical tool anytime you need to assess how likely it is that your result is random.\n",
    "\n",
    "The most common type of null model is one where you shuffle links in your network, while preserving the degree\n",
    "sequence. *(Recall, that the degree sequence is a list that stores the degree of each node. So if we shuffle and\n",
    "preserve the degree sequence at the same time, it means that after all the link-shuffling is done, nodes will\n",
    "have new neighbors, but the same number of neighbors)*. In this shuffled network (the null model), if you find that\n",
    "there are far fewer triangles than in your real data then you can start to argue that your result is significant.\n",
    "\n",
    "> Note: the term *null model* is a slight misnomer, as it is not a model per se, but rather an instance of the\n",
    "data that is permuted in some way (usually under constraints, such as preservation of degree sequence), so it can\n",
    "be taken to represent *randomness*. As such, there is no *model*, but rather an *instance* or simply *data*.\n",
    "\n",
    "But that is just one comparison. What if the number of triangles in the random data–the null model–is smaller, but\n",
    "not that much smaller? Can you still say your result is significant? Well, the trick (although computationally expensive\n",
    "as you will come to learn) is to do MANY comparisons. 1000 is not a bad start. For each comparison, you check if\n",
    "the number of triangles in your real data is bigger. After you have compared one thousand times you compute **the\n",
    "fraction of times** your number of triangles in the real data was bigger than in the random data. Guess what that\n",
    "number (between 0 and 1) is called? **The p-value**. And what does it communicate? **The probability that your result\n",
    "is random!**\n",
    "\n",
    "^ That's some useful statistics right there!\n",
    "\n",
    "Please make sure you have **READ AND UNDERSTOOD** the above, as you will otherwise have a very hard time completing the\n",
    "following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 10.1.1**: The method described above works for anything you might want to measure in a network. Let's say,\n",
    "instead of measuring the number of triangles, you measured the network diameter. Explain in your own words how to\n",
    "assess the statistical significance (the p-value) of such a measurement, using the same null model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If i understand this quetions to be correct - it is how do you assess the p-value, given the null model. This would then be the normal intrepetaion of the p-value, ie, as written, the probability that the model (or in normal econometrics, the estimate) could be the result of a random result. Thus if the p-value is 50%, that would imply a fifty percent chance of the model being the result of random links, in this example, the size of the network diameter. Thus to assess whether the network diameter size is significant, you use the p-value and a threshold to assess whether it is random. Mostly a significance level of 5% is used, this would mean that to consider something significant we want the chance of the model being random to be five percent og less, ie. a p-value below 5. \n",
    "In this specific case, you would compare your diameter size, with sizes of however many null models you have run - and then like in the example compare the amount of times your original model deviated from the random null models to calculate your p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 10.1.2**: The null model described above rewires a network while preserving the degree distribution. [Here](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.swap.double_edge_swap.html#networkx.algorithms.swap.double_edge_swap) is\n",
    "implementation of it in NetworkX. In your own words, describe:\n",
    "1. how it works and why it achieves randomness\n",
    "without changing the degree distribution.\n",
    "2. Also describe what can sometimes happen and why the desired number of swaps `nswaps` is only an upper-bound on\n",
    "the number of swaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** From the small description we get - the swap switches the edge to another note randomly, for each of the links assuring the same degree of distribution\n",
    "**2.** The description show that there is a parameter max tries. This is as sometimes it is not possible to create a new, not before, existing link and thus a max nr. of tries is needed to insure that it doesn't run forever trying to create new links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TAKEN FROM LAST EXCERSICES\n",
    "data = pd.read_csv(\n",
    "    r\"C:\\Users\\frede\\Documents\\GitHub\\Spring2020\\SDS EMML\\facebook-wall.txt.gz\",\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    "    names=['user1', 'user2', 'timestamp']\n",
    ")\n",
    "\n",
    "t0 = data.timestamp.max() - 86400 * 7  # Lower bound of time-slice (here: 7 days before last wall post)\n",
    "t1 = data.timestamp.max()              # Upper bound of time-slice\n",
    "\n",
    "# Subset of the pandas dataframe\n",
    "data_t = data.loc[data.timestamp.between(t0, t1)]\n",
    "\n",
    "# Count the number of times each link occurs and store that in a new 'weight' column\n",
    "data_t = data_t.groupby(['user1', 'user2']).size().reset_index(name='weight')\n",
    "\n",
    "# Create a `nx.DiGraph` from this dataframe\n",
    "G_t = nx.from_pandas_edgelist(data_t, 'user1', 'user2', 'weight', create_using=nx.Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 10.1.3**: Load the Facebook wall-post network from last week, into a `networkx.Graph` object called `G`.\n",
    "1. Measure the average local clustering coefficient (ALCC) of `G`. Print it.\n",
    "2. Over 1000 iterations, measure the ALCC for `G` where, in each iteration, you have made 1000 edge swaps using \n",
    "`double_edge_swap`. Append the measured ALCC values to a list. Print the average of this list. *Note: this will\n",
    "obviously take some time**.\n",
    "3. Report the p-value of your result, the ALCC of the real data.\n",
    "4. Make a histogram that displays the distribution of ALCC values in the null models as a histogram as well as\n",
    "the ALCC of the real data as a vertical line. Comment on this result. Does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01838401670026459\n"
     ]
    }
   ],
   "source": [
    "#1. Find ALLC of G\n",
    "ALLC=nx.average_clustering(G_t)\n",
    "print(ALLC) #0.018 when run first time\n",
    "#2. 1000 iterations\n",
    "val=[]\n",
    "i=0\n",
    "while i<1000:\n",
    "    i=i+1\n",
    "    g_i=G_t\n",
    "    swap=nx.double_edge_swap(g_i,nswap=1000,max_tries=4000)\n",
    "    value=nx.average_clustering(swap)\n",
    "    val.append(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([996.,   0.,   1.,   1.,   0.,   1.,   0.,   0.,   0.,   1.]),\n",
       " array([0.        , 0.0008795 , 0.00175899, 0.00263849, 0.00351798,\n",
       "        0.00439748, 0.00527698, 0.00615647, 0.00703597, 0.00791547,\n",
       "        0.00879496]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3. Calculate p-value\n",
    "# Find the share that diverges more than the original model, so models where the ALCC is larger than of the orignal run on 0.018\n",
    "above=[]\n",
    "for i in val:\n",
    "    if i>0.018:\n",
    "        above.append(1)\n",
    "    else:\n",
    "        above.append(0)\n",
    "        \n",
    "def cal_average(num):\n",
    "    sum_num = 0\n",
    "    for t in num:\n",
    "        sum_num = sum_num + t           \n",
    "\n",
    "    avg = sum_num / len(num)\n",
    "    return avg\n",
    "print(cal_average(above))\n",
    "# It seems that the average (which given they are dummies are also the percentage) of the list is 0, implying a p-value of zero - thus a very significant diffrence.\n",
    "#4. Histogram\n",
    "plt.hist(val)\n",
    "# We see they all have a very low value, which is what we would expect as the because of the zero p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communities are little lumps of nodes in a network that are unusually strongly connected. Your family is a community, and your friend group from work or university is another community. While there is no one true definition of what a community is, there are many many different methods and algorithms for finding them. Here we will work with one of the most popular ones: [Louvain Modularity](https://en.wikipedia.org/wiki/Louvain_Modularity). The following exercises will walk you through the fundamentals of this method, and finally have you apply it to the network you used last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The modularity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.1**: Assume you have a network where nodes and links tend to form lumps here and there. Imagine you now reach for your pen, and start labeling these nodes with group names (or ids) that feel appropriate according to how they are lumped together. If your partition is \"good\", nodes that are connected in groups should intuitively have the same label, while nodes that are distant and disconnected should have different labels. Modularity is a function that can be used to measure, by this logic, *how good* your partition is. It is in technical terms a *utility function*, and it looks like this:\n",
    "> <br><br>\n",
    "> $$ Q = \\dfrac{1}{2m}\\sum_{ij}{\\left[A_{ij}-\\dfrac{k_ik_j}{2m}\\right]\\delta(c_i, c_j)}.$$\n",
    "> <br>\n",
    "> Your job in this problem is to explain this equation. When I look at daunting math I find it calming to try and read it as if it were code. Since all math can be implemented in code, all math can be broken into parts, where each part does a seperate thing. Answer each question below seperately:\n",
    "1. In code, a sum, $\\sum$, is like a `for` loop, where in every iteration you increment a variable. In the equation for modularity the little $ij$ subscript tells is what the sum is looping over (like `for ij in sumloop`). But what is $ij$?\n",
    "2. In each iteration of the sum, the delta function $\\delta(c_i, c_j)$ is used, where $c_i$ is the community label of node $i$. The delta function is a very simple program that returns 0 if the two input values are different and 1 if they are they same. How would you implement the delta function in code? What is it used for in the modularity equation?\n",
    "3. Inside the sum we use the term $\\frac{k_ik_j}{2m}$ as our *null model*. $k$ is the degree sequence (so $k_i$ is the degree of node $i$) and $m$ is the sum of all link weights. Explain what this null model measures. Could we have used other null models?\n",
    "4. The sum subtracts the null model from $A_{ij}$ and adds the result to its final value if the delta function evaluates to 1. What is the point of only summing over this difference when the delta function is 1?\n",
    "5. The sum term is normalized by $2m$. Why exactly $2m$?\n",
    "6. Summarize your insight gained from answering the above questions. In your own words, explain how the modularity function works. Use 1-3 sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. it is the denotaion of the two nodes which compatability in the cluster are being examind with the modularity equation. \n",
    "2. Would make a function that takes two input, store value that is 1 if the two inputs equal eachother and 0 if different, and return the stored value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(a,b):\n",
    "    if a==b: \n",
    "        x=1\n",
    "    else:\n",
    "        x=0 \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We are told that k is the degree sequence and m is the sum of all link weights. Given they are multiplied together, divided by a sum it seems to be a kind of weight of the two nodes interacting. \n",
    "4. We want to examine the level of our partition, so we are only interested in the compatability of nodes that have been partioned into the same group, i.e. the nodes where delta equals to one\n",
    "5. It is two times the sum of all link wieghts, as written in question 3. \n",
    "6. The function estimates how good a partition of data into communities is. It sums through the nodes, i and j and then compares the null model with the model A_ij to see how much it diverges. As it is only interested to look at the compability of the nodes inside the same community, it only sums over the nodes inside the same clusters, thus only saving the information of the pairs of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.2**: Implement the modularity function. Write a Python function that takes as input an adjacency matrix and a label vector, and returns the modularity. Compute and print the modularity for the ones given below. The correct result is 0.122."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T10:27:43.909671Z",
     "start_time": "2019-03-12T10:27:43.900257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12244897959183675\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "])\n",
    "\n",
    "c = [0, 0, 0, 0, 1, 1]\n",
    "\n",
    "def modularity(A, c):\n",
    "    n=A.shape[0]\n",
    "    Q=0\n",
    "    m=A.sum()/2\n",
    "    for i in range(n):\n",
    "        ki=A[i,:].sum()\n",
    "        for j in range(n):\n",
    "            kj=A[:,j].sum()\n",
    "            Q+=(A[i,j]-ki*kj/(2*m))*dummy(c[i],c[j])\n",
    "    return Q/(2*m)\n",
    "print(modularity(A,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.3**: The example labeling, `c`, was not optimal. Find the optimal one and print its modularity score.\n",
    "\n",
    ">*Hint: Either just try a bunch of different label combinations or visualize the network so you can see what is optimal. Using pen and paper here is no shame.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we are now able to evaluate the quality of a partition. But how do we find the best partition? Modularity gives us a way to measure *how good* our partition is, but it does not tell of how to find the best one. For that we need some sort of algorithm. The *Louvain method* is that algorithm.\n",
    "\n",
    "It works in the following steps:\n",
    "1. Set every node to be its own community (initiate `c = [0, 1, 2, 3, 4, 5]`).\n",
    "2. Compute the modularity.\n",
    "3. Now pick a random node.\n",
    "    1. For every neighbor it has, try giving it the neighbor's label, and compute the change in modularity.\n",
    "    2. If any of those relabelings led to an increase in modularity, choose the relabeling with the greatest increase.\n",
    "4. Repeat 2-3 until modularity ceases to increase for any relabelings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.BONUS**: Implement the Louvain method, and show that it gives the labeling for `A`. A cool portfolio project for your Github account here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communication communities on Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with some real data. Whip out the network you created **last week**, we will be using that again. Apply again the **threshold** you created in **Ex. 9.2.4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-15 05:31:31 2009-01-22 05:31:31\n"
     ]
    }
   ],
   "source": [
    "# Code from writeup last week\n",
    "from datetime import datetime as dt\n",
    "print(dt.fromtimestamp(t0),dt.fromtimestamp(t1))\n",
    "def create_slice(data,t0,t1):\n",
    "    if isinstance(t0,str):\n",
    "        t0=dt.timestamp(dt.fromisoformat(t0))\n",
    "    if isinstance(t1,str):\n",
    "        t1=dt.timestamp(dt.fromisoformat(t1))\n",
    "    slice=data.loc[data.timestamp.between(t0,t1)]\n",
    "    slice=slice.groupby(['user1','user2']).size().reset_index(name='Weight')\n",
    "    return nx.from_pandas_edgelist(slice, 'user1','user2','Weight',create_using=nx.Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.4**: Find the communities in this networks. Print the number of communities and plot the distribution of community sizes. See something interesting? Comment on this distribution.\n",
    "\n",
    ">*Hint: You're welcome to use your own implementation of the Louvain algorithm (pretty badass if you can), but there's also a widely used Python implementation that you can take off the shelf. Go ahead and install `python-louvain` by running `conda install -c auto python-louvain` in a terminal. After installation, import it with `import community`, and use `community.best_partition` to get a node-community dictionary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function from last week, same data\n",
    "import community\n",
    "net=create_slice(data,t0,t1)\n",
    "com=community.best_partition(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.5**: Visualize the network, similarly to how you did it last week but this time coloring the nodes by their labels.\n",
    ">\n",
    "> *Hint: [Here](https://netwulf.readthedocs.io/) are the `netwulf` docs. Fiddle around with the layout a little it always makes the network look nicer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netwulf as nw\n",
    "nx.set_node_attributes(net, com, 'group')\n",
    "nw.visualize(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.6:** Maybe the communities you observed in the previous exercise were not as pretty as you were hoping.\n",
    "Admittedly, the Facebook wallpost network is not the most modular network anyway, but still it examplifies a serious\n",
    "problem with the Modularity score as a utility function for community detection. Can you explain what this problem is,\n",
    "and why it becomes increasingly severe as the network grows larger?\n",
    ">\n",
    "> *Hint: it has something to do with the null model that Modularity uses*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not look very good, except for one cluste (darkblue on my pc) all the way to the left, they are all mostly floating around by themselves. The null model that the Modularity function uses is the two nodes degree sequences multiplied divided by a weight sum. This means that the amount of degrees the two nodes have together is very important in the null model - thus also important in the network. So the most connected are easiler grouped, but as facebook connections are many but not too connected it results in this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Final note: there are many other community detection algorithms out there. Check out the [NetworkX docs](https://networkx.github.io/documentation/stable/reference/algorithms/community.html)\n",
    "for some easy-to-use alternatives to Modularity. Also you may want to have a look at [Infomap](https://mapequation.github.io/infomap/python/)\n",
    "especially if you are working with networks where links represent flow (like transactions, exchange, citations, hyperlinks, etc). It usually gives amazing results.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
