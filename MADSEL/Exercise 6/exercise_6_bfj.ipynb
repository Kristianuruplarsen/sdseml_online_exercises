{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - linear ML for econometrics. \n",
    "\n",
    "In this exercise set we will work with linear ML methods that can give unbiased estimates when the number of covariates is large. We will once again set up simulated data to clearly see any issues with the methods we apply. The exercises follow the approach laid out in [Chernozhukov et al](https://arxiv.org/pdf/1501.03185.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following code simulates two datasets, one to use with the auxilliary regression \n",
    ">$$ \\tag{aux}\n",
    "d_i = x_i'\\gamma_0 + z_i' \\delta_0 + u_i\n",
    "$$\n",
    ">and one to use in the main equation\n",
    ">$$ \\tag{main}\n",
    "y_i = \\alpha_0 d_i + x_i' \\beta_0 + \\nu_i\n",
    "$$\n",
    "Unlike in the paper, we include covariates in the main equation. We will handle them in a later question. In this setup $u_i$ and $\\nu_i$ are correlated, meaning the relation between $d_i$ and $y_i$ is not identified via OLS. Instead we will use $z_i$ to induce exogenous variation in $d_i$, which is unrelated to the error terms. This variation can be used to identify $\\alpha_0$.\n",
    ">\n",
    "> **Ex 6.1.1.**  Rewrite the below code to define a function `simulate(n, m, k, plot)`, where plot is a boolean indicating whether the density plot should be drawn or not. Your function should return all the matrices and vectors required in the regression equations, including parameters and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(n=1000, m=1500, k=10, plot=False):\n",
    "    cov = 5    \n",
    "    errors = np.random.multivariate_normal(mean = [0,0], cov = [[cov, cov], [cov, cov]], size = n)\n",
    "    \n",
    "    if plot is True:\n",
    "        h = sns.jointplot(errors[:,0], errors[:,1], kind = 'kde')\n",
    "        h.set_axis_labels('$\\\\nu$', '$\\epsilon$', fontsize=16)\n",
    "\n",
    "    z = np.random.normal(size = (n,m))\n",
    "    x = np.random.normal(size = (n,k))\n",
    "\n",
    "    # Auxilliary equation\n",
    "    nu = errors[:,0]\n",
    "    Pi = np.array([5] + [x if np.random.uniform() > 0.9 else 0 for x in np.random.uniform(low = -2, high = 5, size = m - 1)])\n",
    "    gamma = np.array([5] + [x if np.random.uniform() > 0.9 else 0 for x in np.random.uniform(low = -2, high = 5, size = k - 1)])\n",
    "\n",
    "    d = z @ Pi + x @ gamma + nu\n",
    "\n",
    "    # Main equation\n",
    "    u = errors[:,1]\n",
    "    delta = np.array([5] + [x  if np.random.uniform() > 0.9 else 0 for x in np.random.uniform(low = -2, high = 5, size = k - 1)])\n",
    "    alpha = np.random.uniform(1,2)\n",
    "\n",
    "    y = alpha * d  + x @ delta + u\n",
    "\n",
    "    #Create dict for simulated values\n",
    "    \n",
    "    values = [y, alpha, d, x, delta, u, z, Pi, gamma, nu]\n",
    "    var_names = ['y', 'alpha', 'd', 'x', 'delta', 'u', 'z', 'Pi', 'gamma', 'nu']\n",
    "\n",
    "    return {\"values\":values, \"var_names\":var_names} \n",
    "\n",
    "simulated_res = simulate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Your solution here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.2:** Use your function to simulate a new dataset and regress the following OLS regression\n",
    ">$$\n",
    "y_i \\sim \\pi_0 + \\pi_1 d_i + \\gamma_i\n",
    "$$\n",
    "> where $\\gamma_i$ is a noise term. \n",
    ">\n",
    "> Repeat this procedure 1000 times in a for loop and store the true $\\alpha_0$ as well as the estimate $\\pi_1$ in two lists. Plot a histogram of the differences $\\alpha_0 - \\pi_1$. What does this tell you about the regression you just ran?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "from statsmodels.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   1.,   3.,  19., 110., 292., 363., 172.,  34.,   5.]),\n",
       " array([-0.07092622, -0.06433139, -0.05773656, -0.05114173, -0.04454691,\n",
       "        -0.03795208, -0.03135725, -0.02476243, -0.0181676 , -0.01157277,\n",
       "        -0.00497794]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ex 6.1.2 \n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "# Loop n times\n",
    "n = 1000\n",
    "\n",
    "# Store estimates\n",
    "Pi0_hat = []\n",
    "Pi1_hat = []\n",
    "alpha_hat = []\n",
    "\n",
    "# For loop\n",
    "for x in range(n):\n",
    "\n",
    "    simulated_res = simulate()\n",
    "\n",
    "    alpha = simulated_res[\"values\"][1]\n",
    "    d = simulated_res[\"values\"][2]\n",
    "    y = simulated_res[\"values\"][0]\n",
    "    gamma = np.random.normal(0, 1, size = len(d))\n",
    "\n",
    "    X = np.array([d, gamma]).T #Needs to transpose X\n",
    "\n",
    "    # Our model needs an intercept so we add a column of 1s\n",
    "    X = add_constant(X)\n",
    "    \n",
    "    # OLS regression\n",
    "    model = OLS(y, X)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Append results n times \n",
    "    Pi0_hat.append(results.params[0])\n",
    "    Pi1_hat.append(results.params[1])\n",
    "    alpha_hat.append(alpha)\n",
    "    \n",
    "    # Difference between alpha_0 and Pi1\n",
    "    diff = [x1 - x2 for (x1, x2) in zip(alpha_hat, Pi1_hat)]\n",
    "    \n",
    "plt.hist(diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.3:** Knowing the DGP an obvious solution would be to run an IV regression, instrumenting $d_i$ with $z_i$. Unfortunately there are $m=1500$ columns in $z_i$ and only $n=1000$ observations. Luckily, the way we have simulated our data only a small subset of the $z_i$'s actually influence $d_i$. The tricky part will be to pick out the right $z_i$'s.\n",
    ">\n",
    "> To begin with simulate a new dataset and count the number of non-zero element in $\\Pi$ to verify that indeed only very few $z$'s matter for $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "# Ex 6.1.3 \n",
    "\n",
    "# Simulate a new dataset and store z \n",
    "simulated_res = simulate()\n",
    "Pi = simulated_res[\"values\"][7]\n",
    "\n",
    "non_zero_elements = np.count_nonzero(Pi)\n",
    "\n",
    "print(non_zero_elements)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.4:** The _ideal_ instrument for $d_i$ is exactly the $z_i$'s which have non-zero coefficients, multiplies by the corresponding true simulated parameters in $\\delta_0$. Having simulated the data ourselves, an easy way to compute this is to simply calculate\n",
    "> $$\n",
    "\\hat{d}^* = z \\cdot \\delta_0\n",
    "$$\n",
    "> where $\\cdot$ is the dot product, written as `@` in numpy. In reality we cannot get this ideal instrument, because it would require regressing $d_i$ on all 500 variables with only 100 observations.  \n",
    ">\n",
    "> In a for loop over 1000 iterations, simulate new data, compute the ideal instrument $\\hat{d_i}$ and regress the second stage regression $y_i \\sim \\pi_0 + \\pi_1\\hat{d_i}$. Store the true $\\alpha_0$ and the estimate $\\hat{\\pi}_1$ in two lists. Finally draw a histogram of the differences $\\alpha_0 - \\hat{\\pi}_1$. How does your histogram look this time, is this expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 11.,  33.,  87., 209., 278., 189., 123.,  47.,  18.,   5.]),\n",
       " array([-0.04413459, -0.03463588, -0.02513716, -0.01563845, -0.00613974,\n",
       "         0.00335897,  0.01285768,  0.0223564 ,  0.03185511,  0.04135382,\n",
       "         0.05085253]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ex 6.1.4 \n",
    "    \n",
    "# Store true parameters \n",
    "Pi1_hat = []\n",
    "alpha_hat = []\n",
    "\n",
    "# For loop\n",
    "n = 1000\n",
    "\n",
    "for x in range(n):\n",
    "    simulated_res = simulate()\n",
    "\n",
    "    alpha = simulated_res[\"values\"][1]\n",
    "    y = simulated_res[\"values\"][0]\n",
    "    Pi = simulated_res[\"values\"][7]\n",
    "    z = simulated_res[\"values\"][6]\n",
    "    d_hat = z @ Pi #mismatch between text and code\n",
    "\n",
    "    X = d_hat\n",
    "\n",
    "    # Our model needs an intercept so we add a column of 1s\n",
    "    X = add_constant(X)\n",
    "    \n",
    "    # OLS regression\n",
    "    model = OLS(y, X)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Append results n times \n",
    "    Pi1_hat.append(results.params[1])\n",
    "    alpha_hat.append(alpha)\n",
    "    \n",
    "    \n",
    "    # Difference between alpha_0 and Pi1\n",
    "    diff = [x1 - x2 for (x1, x2) in zip(alpha_hat, Pi1_hat)]\n",
    "    \n",
    "plt.hist(diff)\n",
    "\n",
    "# The difference is now located around zero as expected. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.5:** The below class implements post-lasso. A two step procedure where first a lasso model is used to identify relevant parameters, and then OLS is used to estimate parameter values on the remaining variables. Study the code, and understand as well as possible what is going on. \n",
    ">\n",
    "> What is stored in `relevant_x`? In `relevant_x` all the relevant exogenous variables chosen by the lasso model are stored. The lasso model chooses all the coefs different from 0. \n",
    "\n",
    "> \n",
    "> Why is the `predict` method so complicated? The reason for using the if functions is to allow the user to be able to specify other columns and to secure dimensionality between columns.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostLasso:\n",
    "    def __init__(self, formula = None):\n",
    "        self.lasso_model = Lasso()\n",
    "        self.ols_model = None\n",
    "        self.relevant_x = None\n",
    "        self.subset_cols = None\n",
    "        self.coefs = None\n",
    "        self.formula = formula\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'PostLasso({self.formula})'\n",
    "    \n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def fit(self, X, y, force_include_idx = None):\n",
    "        ''' Estimate a model using Post-Lasso\n",
    "        \n",
    "        X: X matrix (without intercept)\n",
    "        y: y vector\n",
    "        force_include_idx: column indexes that ALWAYS is\n",
    "            included in the OLS model, regardless of their\n",
    "            status in the lasso stage.\n",
    "        '''\n",
    "        self.lasso_model = self.lasso_model.fit(X,y)\n",
    "        self.coefs = np.insert(self.lasso_model.coef_, 0, self.lasso_model.intercept_)\n",
    "        self.subset_cols = np.where(self.coefs != 0)[0]\n",
    "        if force_include_idx is not None:\n",
    "            self.subset_cols = np.union1d(self.subset_cols, force_include_idx)\n",
    "        self.relevant_x = add_constant(X)[:,self.subset_cols]\n",
    "        self.ols_model = OLS(y, self.relevant_x).fit()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X = None):\n",
    "        ''' Predict using a fitted post-lasso model.\n",
    "        '''\n",
    "        if X is None:\n",
    "            return self.ols_model.predict(self.relevant_x)\n",
    "        if X.shape == self.relevant_x.shape:\n",
    "            return self.ols_model.predict(X)\n",
    "        return self.ols_model.predict(X[:,self.subset_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.6:** In this problem we will try to run through the post-lasso steps required to obtain an estimate of $\\alpha_0$. Since we are doing this in python we will not be able to set the lasso hyperparameter optimally for this kind of post-selection usage. There is a R package, developed especially to handle inference after lasso-selection, which you should use in practise. \n",
    ">\n",
    "> For now, do the following steps 1000 times, storing the true $\\alpha_0$ and estimate $\\hat{\\alpha_0}$:\n",
    ">\n",
    "> * 0. Simulate a new dataset.\n",
    "> * 1. Run a post-lasso regression of d on x and z, $d_i \\sim x_i'\\gamma + z_i' \\delta$, forcing the inclusion of $x_i$ in the OLS stage.\n",
    "> * 2. Run the second stage regression $y_i \\sim \\hat{d}_i + x_i' \\beta$ to recover $\\hat{\\alpha_0}$.\n",
    ">\n",
    "> How does this histogram compare to the naive one? How does it compare to the ideal one?\n",
    ">\n",
    "> _Hint:_ We follow the description given on page 19 [here](https://cran.r-project.org/web/packages/hdm/vignettes/hdm.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   1.,  25.,  97., 220., 271., 236., 116.,  26.,   7.]),\n",
       " array([-9.96574629e-03, -8.30138980e-03, -6.63703331e-03, -4.97267681e-03,\n",
       "        -3.30832032e-03, -1.64396383e-03,  2.03926657e-05,  1.68474916e-03,\n",
       "         3.34910565e-03,  5.01346215e-03,  6.67781864e-03]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ex 6.1.6\n",
    " \n",
    "# Store true parameters \n",
    "b_hat = []\n",
    "alpha_hat = []\n",
    "\n",
    "# For loop\n",
    "n = 1000\n",
    "\n",
    "# Simulate a new dataset 1000 times\n",
    "for _ in range(n):\n",
    "    simulated_res = simulate()\n",
    "    y = simulated_res[\"values\"][0]\n",
    "    alpha = simulated_res[\"values\"][1]\n",
    "    d = simulated_res[\"values\"][2]\n",
    "    Pi = simulated_res[\"values\"][7]\n",
    "    z = simulated_res[\"values\"][6]\n",
    "    delta = simulated_res[\"values\"][4]\n",
    "    gamma = simulated_res[\"values\"][8]\n",
    "    x = simulated_res[\"values\"][3]\n",
    "    \n",
    "    X = np.c_[x, z] #Needs to transpose X\n",
    "\n",
    "    #Lasso Model \n",
    "    model_lasso = PostLasso('di sim xi zi')\n",
    "    model_lasso.fit(X, d, force_include_idx = np.arange(x.shape[1]))\n",
    "    d_hat = model_lasso.predict()\n",
    "    \n",
    "    #Create X vector for the second stage\n",
    "    X_2sls = np.c_[x,d_hat]\n",
    "    \n",
    "    #Initialize 2SLS model \n",
    "    stage2 = OLS(y, X_2sls).fit()\n",
    "    \n",
    "    #Store estimates\n",
    "    b_hat.append(stage2.params[-1])\n",
    "    alpha_hat.append(alpha) \n",
    "    \n",
    "# Difference  \n",
    "\n",
    "diff = [x1 - x2 for (x1, x2) in zip(alpha_hat, b_hat)]\n",
    "    \n",
    "plt.hist(diff)\n",
    "    \n",
    "# We find an improvement compared to the naive model even though some bias still evident (the estimator is too large). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
